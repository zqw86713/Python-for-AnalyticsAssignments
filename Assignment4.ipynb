{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2023, Qingwei Zhang. All Rights Reserved.\n",
    "#\n",
    "# MSCA 37014 3 Python for Analytics,\n",
    "# Assignment 4: Churn analysis on Reddit\n",
    "#\n",
    "# Student: Qingwei Zhang\n",
    "# Email: qingwei.zhang@uchicago.edu\n",
    "# ==============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pyarrow.feather as feather\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data file location.\n",
    "data_file_location = \"./RC_2012_year_cohort.feather\"\n",
    "\n",
    "# read thefile, and the result is pandas.DataFrame\n",
    "df = feather.read_feather(data_file_location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1325376000</td>\n",
       "      <td>irwinator</td>\n",
       "      <td>AskReddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1325376000</td>\n",
       "      <td>reed311</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1325376000</td>\n",
       "      <td>sagapo3851</td>\n",
       "      <td>gaming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1325376000</td>\n",
       "      <td>filthgrinder</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1325376001</td>\n",
       "      <td>BitterDivorcedDad</td>\n",
       "      <td>WTF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   created_utc             author  subreddit\n",
       "0   1325376000          irwinator  AskReddit\n",
       "1   1325376000            reed311   politics\n",
       "2   1325376000         sagapo3851     gaming\n",
       "3   1325376000       filthgrinder   politics\n",
       "4   1325376001  BitterDivorcedDad        WTF"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the head of the df.\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(234225454, 3)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the shape.\n",
    "df.shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Find the top 30 subreddits\n",
    "\n",
    "Count the number of comments in each subreddit, sort that list and pick the top 30.\n",
    "While you are writing this code, it may be easier just to pick any three smaller sub-reddits. With smaller amount of data, you will be able to develop your code faster. Once you are sure the code works, switch to the top 30 sub-reddits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AskReddit              32023468\n",
       "funny                  11909021\n",
       "pics                    8973606\n",
       "gaming                  6966306\n",
       "WTF                     6377885\n",
       "politics                5397547\n",
       "atheism                 5043438\n",
       "leagueoflegends         4029221\n",
       "IAmA                    3883036\n",
       "trees                   3846708\n",
       "videos                  3611531\n",
       "AdviceAnimals           3571534\n",
       "todayilearned           2858174\n",
       "fffffffuuuuuuuuuuuu     2785323\n",
       "worldnews               2270210\n",
       "gonewild                2168728\n",
       "nfl                     2003374\n",
       "movies                  1745437\n",
       "starcraft               1729070\n",
       "aww                     1691410\n",
       "technology              1503424\n",
       "Minecraft               1491322\n",
       "soccer                  1339712\n",
       "Music                   1146500\n",
       "nba                     1116769\n",
       "Diablo                  1100630\n",
       "mylittlepony            1093572\n",
       "Guildwars2              1080067\n",
       "Games                   1068527\n",
       "guns                    1024160\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set how many subreddit from the top we want to see.\n",
    "ranking = 30\n",
    "\n",
    "df_ranking = df[\"subreddit\"].value_counts(sort=True, ascending=False)[:ranking]\n",
    "\n",
    "df_ranking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainerroad              1\n",
       "VDI                      1\n",
       "nod32                    1\n",
       "RevivalGuild             1\n",
       "onlinecommunities        1\n",
       "paxskeptica              1\n",
       "dacespicks               1\n",
       "Fin                      1\n",
       "serialkillersbookclub    1\n",
       "EatSleepLift             1\n",
       "WesternMovies            1\n",
       "Toyota86                 1\n",
       "CommonLawClass           1\n",
       "whatdoesitallmean        1\n",
       "clivecussler             1\n",
       "BrownUniversity          1\n",
       "yui                      1\n",
       "BaconParadise            1\n",
       "AppleSupport             1\n",
       "Allergy                  1\n",
       "ANIMALHELP               1\n",
       "teflactivities           1\n",
       "DWTP                     1\n",
       "warpprism                1\n",
       "TrueAnimeCSS             1\n",
       "WTFRussia                1\n",
       "dotnet30                 1\n",
       "Rule_34                  1\n",
       "ChildrensLit             1\n",
       "HIV101course             1\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO, will remove, pick the last 3 subreddit.\n",
    "\n",
    "reverse_df_ranking = df[\"subreddit\"].value_counts(sort=True, ascending=True)[\n",
    "    :ranking\n",
    "]\n",
    "\n",
    "reverse_df_ranking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 AskReddit\n",
      "1 funny\n",
      "2 pics\n",
      "3 gaming\n",
      "4 WTF\n",
      "5 politics\n",
      "6 atheism\n",
      "7 leagueoflegends\n",
      "8 IAmA\n",
      "9 trees\n",
      "10 videos\n",
      "11 AdviceAnimals\n",
      "12 todayilearned\n",
      "13 fffffffuuuuuuuuuuuu\n",
      "14 worldnews\n",
      "15 gonewild\n",
      "16 nfl\n",
      "17 movies\n",
      "18 starcraft\n",
      "19 aww\n",
      "20 technology\n",
      "21 Minecraft\n",
      "22 soccer\n",
      "23 Music\n",
      "24 nba\n",
      "25 Diablo\n",
      "26 mylittlepony\n",
      "27 Guildwars2\n",
      "28 Games\n",
      "29 guns\n"
     ]
    }
   ],
   "source": [
    "# Show the index name.\n",
    "for i in range(ranking):\n",
    "    print(i, df_ranking.index[i])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Filter the dataframe so you are only looking at data form a single sub-reddit.\n",
    "\n",
    "After this step, you should have a dataframe which only contains `author` and `created_utc` (you don't need the sub-reddit, because you are only working with a single sub-reddit at a time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_single_subreddit_info(ranking_df, current_index):\n",
    "    single_subreddit = df.loc[\n",
    "        df[\"subreddit\"] == ranking_df.index[current_index]\n",
    "    ]\n",
    "\n",
    "    single_subreddit_df = single_subreddit[[\"created_utc\", \"author\"]]\n",
    "\n",
    "    return single_subreddit_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "\n",
    "# TODO, code to be run at production.\n",
    "# single_subreddit_info = extract_single_subreddit_info(df_ranking,index)\n",
    "\n",
    "single_subreddit_info = extract_single_subreddit_info(\n",
    "    df_ranking, index\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Find the date of first interaction (for that sub-reddit)\n",
    "Since we are looking to calculate metrics per sub-reddit, if `user123` \n",
    "posts a comment on 'gaming' and 'politics', we will consider them to be \n",
    "different authors.\n",
    "\n",
    "Since our analysis will be done on a relative time basis (day 0 and day 1 \n",
    "will be different for different users), calculate the date on which an author posted their first comment to that \n",
    "sub-reddit.\n",
    "\n",
    "This will serve as our \"day zero.\" You should end up with a dataframe \n",
    "containing the author and the date of the first comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to get all unique values of a columns\n",
    "def get_unique_values(single_subreddit_info):\n",
    "    #  get all Unique values of a columns as a list\n",
    "    list_of_author = single_subreddit_info[\"author\"].unique().tolist()\n",
    "\n",
    "    # get the number of distinct values in a column.\n",
    "    number_of_author = len(list_of_author)\n",
    "\n",
    "    return list_of_author, number_of_author\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to find the date of first interaction.\n",
    "def find_first_interaction(single_subreddit_info, iteration, list_of_author):\n",
    "    # build a pandas DataFrame with a for-loop in Python\n",
    "    rows = []\n",
    "\n",
    "    for i in range(iteration):\n",
    "        # get the day zero info for each author, it should be the\n",
    "        # UTC  that has the minumum value. Since it is a single item\n",
    "        # list object, we only need the first element.\n",
    "        day_zero = (\n",
    "            single_subreddit_info[\n",
    "                single_subreddit_info[\"author\"] == list_of_author[i]\n",
    "            ]\n",
    "            .min()\n",
    "            .values[0]\n",
    "        )\n",
    "\n",
    "        # append to the list object.\n",
    "        rows.append([list_of_author[i], day_zero])\n",
    "\n",
    "    day_zero_df = pd.DataFrame(rows, columns=[\"author\", \"day_zero\"])\n",
    "\n",
    "    return day_zero_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1125887 \n",
      " ['irwinator', 'AddictivePotential', 'Solumin', 'AliasSigma', '[deleted]', 'hecticengine', 'homer1969', 'rco8786', 'kagutsuchi', 'elliottblackwood']\n"
     ]
    }
   ],
   "source": [
    "list_of_author, number_of_author = get_unique_values(single_subreddit_info)\n",
    "\n",
    "print(f\" {number_of_author} \\n {list_of_author[:10]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>day_zero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>irwinator</td>\n",
       "      <td>1325376000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AddictivePotential</td>\n",
       "      <td>1325376005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Solumin</td>\n",
       "      <td>1325376006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AliasSigma</td>\n",
       "      <td>1325376012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[deleted]</td>\n",
       "      <td>1325376013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hecticengine</td>\n",
       "      <td>1325376017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>homer1969</td>\n",
       "      <td>1325376024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rco8786</td>\n",
       "      <td>1325376024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>kagutsuchi</td>\n",
       "      <td>1325376025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>elliottblackwood</td>\n",
       "      <td>1325376032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>whatamuffin</td>\n",
       "      <td>1325376033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Papa_Andy</td>\n",
       "      <td>1325376035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Soukai</td>\n",
       "      <td>1325376036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>thealleine</td>\n",
       "      <td>1325376038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>toxicomano</td>\n",
       "      <td>1325376040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>spottedzebra</td>\n",
       "      <td>1325376040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>WeShouldFuck</td>\n",
       "      <td>1325376041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Lunchbox5000</td>\n",
       "      <td>1325376041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>BottmCmmentConfessor</td>\n",
       "      <td>1325376043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>kealoha</td>\n",
       "      <td>1325376047</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  author    day_zero\n",
       "0              irwinator  1325376000\n",
       "1     AddictivePotential  1325376005\n",
       "2                Solumin  1325376006\n",
       "3             AliasSigma  1325376012\n",
       "4              [deleted]  1325376013\n",
       "5           hecticengine  1325376017\n",
       "6              homer1969  1325376024\n",
       "7                rco8786  1325376024\n",
       "8             kagutsuchi  1325376025\n",
       "9       elliottblackwood  1325376032\n",
       "10           whatamuffin  1325376033\n",
       "11             Papa_Andy  1325376035\n",
       "12                Soukai  1325376036\n",
       "13            thealleine  1325376038\n",
       "14            toxicomano  1325376040\n",
       "15          spottedzebra  1325376040\n",
       "16          WeShouldFuck  1325376041\n",
       "17          Lunchbox5000  1325376041\n",
       "18  BottmCmmentConfessor  1325376043\n",
       "19               kealoha  1325376047"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = find_first_interaction(single_subreddit_info, 20, list_of_author)\n",
    "\n",
    "test_df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Combine the initial dataframe with the dataframe containing comment timestamps\n",
    "After the second step, you had a dataframe containing the name of the author who commented and the time in which they commented. Combine this with the dataframe from step 2, where you created a dataframe which contains the name of the author who commented and the time when they _first__ commented (in that sub-reddit).\n",
    "\n",
    "You can combine these two dataframes using the `merge` function (among other methods). At a high level, you are able to combine the two dataframes because they both contain the `author` column.\n",
    "\n",
    "Note that we don't need hour/minute/second information. We are only concerned about their daily activity. You are welcome to use the `created_utc` field, as it exists, but this assignment will likely be easier if you use `pd.to_datetime` to convert to an actual `datetime` type.\n",
    "\n",
    "You should now have a dataframe which contains the author's name, the date of their first comment and the date of their current comment.\n",
    "\n",
    "Use the two dates in  your new dataframe to calculate the number of days since the first comment. \n",
    "\n",
    "You no longer need the original dates. You can do the rest of your analysis just with the author name and the column containing the days (since day zero) on which the author made a comment.\n",
    "\n",
    "Note that many authors will comment in a subreddit multiple times in a day. We don't care about multiple comments. We are only care if they commented in a sub-reddit _at least_ once on that day."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hmi",
   "language": "python",
   "name": "hmi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
